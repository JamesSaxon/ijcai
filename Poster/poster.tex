\documentclass[final]{beamer}
\usepackage{amsmath,amsthm,amssymb}

% \usepackage{tikz}
% \usepackage{wrapfig}
% \usepackage{algorithm}
% \usepackage{algorithmic}

\include{mydef}
\input{macros}
% \usetikzlibrary{calc}

\usetheme{ubc}
\usepackage[orientation=portrait,size=a0,scale=1.22,debug]{beamerposter}

\newtheorem{mydefinition}{Definition}
\newtheorem{proposition}[mydefinition]{Proposition}

\newcommand{\xbest}{\mathbf{\vx}^{+}}

\newcommand{\vectornorm}[1]{\left|\left|#1\right|\right|}
\def\sigman{\sigma}
\newcommand{\SSR}{\mbox{SSR}}
\newcommand{\murf}{\mu_{\mathtt{rf}}}
\newcommand{\sigrf}{\sigma^2_{\mathtt{rf}}}
\newcommand{\eirf}{EI_{\mathtt{rf}}}
\newcommand{\func}{(\cdot)}
\newcommand{\tree}{\mathcal{T}}
\newcommand{\fmap}{\widehat{\mathbf{f}}}
\newcommand{\vxstar}{\mathbf{x}^{\star}}
\newcommand{\vxbest}{\mathbf{x}^{+}}
\newcommand\TT{\rule{0pt}{2.6ex}}
\newcommand\BB{\rule[-1.2ex]{0pt}{0pt}}
\newcommand{\FIM}{{\bf J}}
\DeclareMathOperator*{\argmax}{argmax}
% \DeclareMathOperator*{\argmin}{argmin}
\newcommand{\sfrac}[2]{\leavevmode\kern.1em
           \raise.5ex\hbox{\footnotesize #1}\kern-.1em
                   /\kern-.15em\lower.25ex\hbox{\footnotesize #2}}

\def\mnote#1{\marginpar{\tiny #1}}
\def\rmnote#1{\reversemarginpar{\tiny #1}}
\def\capstyle#1{\small \emph{#1}}


\definecolor{myColor}{rgb}{0.1,0.0,0.8}

%===============================================================================
\title{Bayesian Optimization in High Dimensions via Random Embeddings}
\author{Ziyu Wang, Masrour Zoghi, Frank Hutter, 
David Matheson, Nando de Freitas}
% \institute{
% Computer Science Department, University of British Columbia}
% \homepage{http:/www.cs.ubc.ca/\string~ziyuw}

\begin{document}
\begin{frame}[t]
\begin{columns}[T]
\begin{column}{.48\textwidth}

% COLUMN 1.
%===============================================================================
\begin{block}{Background}
 \begin{minipage}[r]{0.62\columnwidth}
  {\bf \textcolor{myColor}{Bayesian Optimization (BO)}}
 addresses the following global optimization problem
 \[ \vx^{\star} = \argmax_{\vx \in {\cal X}} f(\vx). \]
 We are particularly interested in objective functions $f$ 
 that may satisfy one or more of the following criteria: 
 \begin{itemize}
  \item noisy,
  \item expensive to evaluate,
  \item do not have easily available derivatives.
 \end{itemize}
 \begin{figure}
    \centering
    \includegraphics[width=0.55\columnwidth]
    {../Presentation/figs/blackbox}
    \label{fig:traj}
  \end{figure}
 BO is a simple algorithm and it repeats the following three steps:
   \begin{enumerate}
    \item Use the prior to decide at which input $x \in \cal X$ to query $f$ next
    by optimizing {\bf \textcolor{myColor}{acquisition functions}};
    \item Evaluate $f(x)$;
    \item Update the prior based on the new data 
    $\langle{}y, f(x)\rangle$.
   \end{enumerate} 
  \end{minipage}
  \begin{minipage}[l]{0.37\columnwidth}
  \begin{figure}
   \begin{itemize}
    \item[] \includegraphics[width=0.8\columnwidth]{../Presentation/figs/bo1}
    \item[] \includegraphics[width=0.8\columnwidth]{../Presentation/figs/bo2}
    \item[] \includegraphics[width=0.8\columnwidth]{../Presentation/figs/bo3}
   \end{itemize}
   \caption{Demonstration of Bayesian Optimization. The dashed line is the true
  objective function. The solid line is the posterior mean while the
  purple region represent $1$ standard deviation. The green region is these
  acquisition function the maximizer of which is chosen to be the next point
  to evaluate.}
  \end{figure}
  \end{minipage}
  \\~\\~\\
  \begin{minipage}[l]{0.63\columnwidth}
  The Rise of Bayesian Optimization: 
     \begin{itemize}
   \item Algorithm Configuration
   \begin{itemize}
    \item Automatic ML.
    \item SAT solver
    \item Programming by optimization
    \item etc.
   \end{itemize}
   \item Animation and intelligent UI
   \item A/B testing.
   \item Control.
   \item etc.
   \end{itemize}
    \end{minipage}
  \begin{minipage}[r]{0.35\columnwidth}
     \begin{figure}[t]
      \includegraphics[width=0.45\columnwidth]{../Presentation/figs/animation}  
      \includegraphics[width=0.55\columnwidth]{../Presentation/figs/wine_arms_EI_T40} \\
      \includegraphics[width=0.45\columnwidth]{../Presentation/figs/abtest}
      \includegraphics[width=0.55\columnwidth]{../Presentation/figs/control}
     \end{figure}
    \end{minipage}
  
 \begin{itemize}
    \item {\bf \textcolor{red}{ Challenge -- Curse of Dimensionality}}
   \end{itemize}
\end{block}

\begin{block}{Algorithm}

     \begin{itemize}
      \item Often, only few dimensions change the objective function significantly
       \textcolor{gray}{\cite{Bergstra:2012, Hutter:2013_KeyParameters}}.
      \item That is to say these problems have 
       {\bf \textcolor{myColor}{low effective dimensionality}}.
       \item How can we take advantage of this?
     \end{itemize}

    
    The idea:
   \begin{itemize}
    \item {\bf \textcolor{myColor}{Embed}} 
     a low dimensional space into the high dimensional one
    \item Optimize
     only on the low dimensional space.
   \end{itemize}

  \begin{figure}[t]
   \includegraphics[width = 0.9\columnwidth]
   {../paper/figures/2to1embedding}
   \label{fig:ESSL_BLR}
  \end{figure}
  
   
  \begin{block}{REMBO}
    \begin{itemize}
   \item Choose compact set {\bf \textcolor{myColor}{$\mathbf{\mathcal{Y}}$}}.
   \item Draw a random Gaussian matrix \textcolor{myColor}{$\mathbf{A}$}.
   \item Repeat:
   \begin{enumerate}
    \item Use the prior to decide at which input $y \in \cal Y$ to query $f$ next
    by optimizing acquisition functions;
    \item Evaluate {\bf \textcolor{myColor}{$\mathbf{f(Ay)}$}};
    \item Update the prior based on the new data 
    {\bf \textcolor{myColor}{$\mathbf{\langle{}y, f(Ay)\rangle}$}}.
   \end{enumerate}
  \end{itemize}
  \end{block}
  
  \begin{figure}
\centering
  \includegraphics[scale=0.65]{../paper/figures/projection.pdf}
  \caption{Embedding from $d=1$ into $D=2$. The box illustrates the 2D constrained space ${\cal X}$, while the thicker red line illustrates the 1D constrained space $\mathcal{Y}$. Note that if $\vA\vy$ is outside $\mathcal{X}$, it is projected onto $\mathcal{X}$. The set $\mathcal{Y}$ must be chosen large enough so that the projection of its image, $\vA \mathcal{Y}$, onto the effective subspace (vertical axis in this diagram) covers the vertical side of the box.}

\end{figure}

\end{block}

\begin{block}{Selected references}
 \bibliographystyle{plain}
  \vspace{-.8em}
  \bibliography{../paper/bayesopt}
\end{block}

\end{column}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% New Column
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{column}{.48\textwidth}
\begin{block}{Guarantees}
\begin{itemize}
 \item Given a large enough $\cal Y$, we can locate the optimizer in the low dimensional space with probability $1$:
 
\end{itemize}
  \begin{theorem}
   \label{prop:1}
   Assume we are given a function $f: \mathbb{R}^{D} \rightarrow \mathbb{R}$ with effective dimensionality $d_e$ and a random Gaussian matrix $\vA \in \mathbb{R}^{D\times d}$. Then, with probability 1, for any $\vx \in \mathbb{R}^D$, there exists a $ \vy \in \mathbb{R}^d$ such that $f(\vx) = f(\vA\vy)$.
  \end{theorem}

%   \begin{itemize}
%    \item The previous theorem suggest that given a large enough $\cal Y$, we can locate the optimizer in the low dimensional space.
%    \item The next theorem guarantees that $\mathcal{Y}$ can be chosen to be small and still contain the optimum with probability at least $1-\epsilon$.
%   \end{itemize}
\begin{itemize}
 \item $\mathcal{Y}$ can be chosen to be small and still contain the optimum with reasonable probability:
\end{itemize}
 \begin{theorem}
   \label{prop:2}
   Given a box $\cal X$ centered around $\mathbf{0}$, if $\vA$ is a $D\times d$ random Gaussian matrix 
   with independent entries,
   there exists an optimizer $\vy^\star \in \mathbb{R}^{d}$ such that $f(\vA\vy^\star) = f(\vx^\star_\top)$ and $$\|\vy^\star\|_2 \leq \frac{\sqrt{d_e}}{\epsilon}\|\vx^{\star}_\top\|_2$$ with probability at least $1-\epsilon$ where $d_e$ is
   the effective dimension.
 \end{theorem}
\end{block}

\begin{block}{Low vs. High Dimensional Kernel}
 \begin{minipage}[l]{0.57\columnwidth}
    \begin{itemize}
    \small
   \item {\bf \textcolor{myColor}{low-dimensional kernel}} (defined on 
   {\bf \textcolor{myColor}{$\cal Y$}}):\\
   $$k_{\ell}^d(\vy^{(1)},\vy^{(2)}) = \exp\left({-\frac{\|\vy^{(1)}-\vy^{(2)}\|^2}{2\ell^2}}\right). $$
  \end{itemize}
  \begin{itemize}
 \small
  \item {\bf \textcolor{myColor}{high-dimensional kernel}} (defined on 
   {\bf \textcolor{myColor}{$\cal X$}}):  
  $$k_{\ell}^D(\vy^{(1)}, \vy^2) = \exp\left( -\frac{\| p_{\mathcal{X}}(\vA\vy^{(1)}) - p_{\mathcal{X}}(\vA\vy^{(2)}) \|^2}{2\ell^2} \right),$$
  where $p_{\mathcal{X}}:\mathbb{R}^D \rightarrow \mathbb{R}^D$ is the standard projection operator for our box-constraint: $p_{\mathcal{X}}(\vy) = {\arg \min}_{\vz\in \mathcal{X}} \|\vz-\vy\|_2$. 
  \end{itemize}
  \end{minipage}
 \begin{minipage}[r]{0.42\columnwidth}
  \begin{figure}
  \begin{flushright}
   \includegraphics[width=1\columnwidth]{../Presentation/figs/low_high_dim.png}
   \end{flushright}
  \end{figure}
 \end{minipage}
 
\end{block}

\begin{block}{Synthetic Experiments}

  \begin{itemize}
   \item We embed the 2-dimensional Branin function in D-dimensional spaces to 
   construct synthetic high-dimensional function.
   \item We first try D=25 then increase D to be 1,000,000,000.
   \item We also rotate the constructed synthetic function.
   \item We compare to two other approaches HD BO~\cite{chen:2012} and random search~\cite{Bergstra:2012}.
  \end{itemize}
  
\begin{figure}[t!]
  \includegraphics[scale=0.9]{../paper/figures/branin_dis_25.png}
  \includegraphics[scale=0.9]{../paper/figures/branin_dis_rot.png}
  \includegraphics[scale=0.9]{../paper/figures/branin_dis_1b.png}
  \caption{Comparison of random search (RANDOM), Bayesian optimization (BO),
   HD BO, and REMBO.
Left: $D=25$ extrinsic dimensions; Middle: $D=25$, with a rotated objective function; Right: $D=10^9$ extrinsic dimensions. We plot means and $1/4$ standard deviation confidence intervals of the optimality gap across 50 trials.}
  \label{fig:standard}
\end{figure}
\end{block}

\begin{block}{Configuration of lpsolve}
Automatic Configuration of a Mixed Integer
Linear Programming Solver:
\begin{itemize}
   \item Downloaded over 40000 times in the past year.
   \item Has 47 discrete parameters.
   \item Also compared against SMAC and ParamILS which are state of the art algorithms
   for algorithm configuration.
  \end{itemize}
 \begin{figure}[h!]
\begin{center}
  \includegraphics[scale=0.8]{../paper/figures/lpsolve.png}
  \includegraphics[scale=0.8]{../paper/figures/lpsolve_interleave.png}
  \caption{Performance for configuration of \texttt{lpsolve}; we show the optimality gap \texttt{lpsolve} achieved with the configurations found by the various methods (lower is better). Top: a single run of each method; Bottom: performance with $k=4$ interleaved runs. We plot means and $1/4$ standard deviations over 20 repetitions of the experiment.
        %results of 20  confidence. Note that the standard deviation of the mean would be slightly smaller than the ones plotted as we repeat the experiment 20 times.
        }
\label{fig:lpsolve}
\end{center}
\end{figure}
\end{block}


\begin{block}{Conclusions}
 \begin{itemize}
   \item BO is a very efficient algorithm and has many applications.
   \item It, however, is limited to moderate dimensionality.
   \item In this paper we introduced REMBO which only requires a simple modification of the original BO algorithm.
   \item The performance of REMBO depends solely on the effective dimension and it is rotation invariant.
   \item Code is available at: https:/github.com/ziyuw/rembo
  \end{itemize}
\end{block}



%===============================================================================
\end{column}
\end{columns}
\end{frame}
\end{document}

